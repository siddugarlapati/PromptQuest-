from fastapi import APIRouter
from pydantic import BaseModel
from typing import Optional
from simulation import ollama_client
from simulation.attention import simulate_attention
from simulation.confidence import simulate_confidence
from simulation.prompt_scorer import score_prompt

router = APIRouter()


class GenerateRequest(BaseModel):
    prompt: str
    model: Optional[str] = None
    system: Optional[str] = "You are a helpful AI teacher explaining concepts clearly and simply."
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = 512


class CompareRequest(BaseModel):
    prompt: str
    model: Optional[str] = None
    system: Optional[str] = "You are a helpful AI teacher. Answer clearly and concisely."
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = 512


@router.get("/status")
async def ollama_status():
    """Check if Ollama is running and get available models."""
    available = await ollama_client.is_ollama_available()
    models = await ollama_client.get_available_models() if available else []
    return {
        "available": available,
        "models": models,
        "suggested_model": models[0] if models else "llama3.2",
        "install_cmd": "ollama pull llama3.2" if not models else None,
        "message": "Ollama is running ✅" if available else "Ollama not found. Start with: ollama serve"
    }


@router.post("/generate")
async def generate_with_ollama(req: GenerateRequest):
    """Generate a response using local Ollama LLM."""
    result = await ollama_client.generate(
        req.prompt, 
        model=req.model, 
        system=req.system,
        temperature=req.temperature,
        max_tokens=req.max_tokens
    )
    return result


@router.post("/compare")
async def compare_simulation_vs_real(req: CompareRequest):
    """
    Compare simulation output vs real Ollama LLM output side by side.
    """
    # Simulation output
    score_data = score_prompt(req.prompt)
    sim_output = _generate_simulation_response(req.prompt, score_data)

    # Real AI output
    real_result = await ollama_client.generate(
        req.prompt,
        model=req.model,
        system=req.system,
        temperature=req.temperature,
        max_tokens=req.max_tokens
    )

    # Attention on prompt
    attention = simulate_attention(req.prompt)

    return {
        "prompt": req.prompt,
        "simulation": {
            "response": sim_output,
            "type": "Rule-based simulation",
            "explanation": "Generated using pre-defined rules and heuristics — no real understanding.",
            "word_count": len(sim_output.split()),
        },
        "real_ai": {
            "response": real_result["response"] if real_result["success"] else None,
            "available": real_result["success"],
            "model": real_result["model"],
            "error": real_result.get("error"),
            "type": "Local LLM via Ollama",
            "explanation": "Generated by a real language model with billions of parameters.",
            "word_count": len(real_result["response"].split()) if real_result["success"] else 0,
        },
        "attention": attention,
        "insight": (
            "Notice how the Real AI gives a richer, more contextual answer while simulation gives a template response. "
            "This is because real LLMs have learned from billions of documents!"
        ) if real_result["success"] else (
            "Install Ollama to see real AI responses and compare them with simulation!"
        )
    }


def _generate_simulation_response(prompt: str, score_data: dict) -> str:
    """Generate a simple template-based simulation response."""
    prompt_lower = prompt.lower()
    if any(w in prompt_lower for w in ["explain", "what is", "describe"]):
        topic = prompt.split()[-1] if prompt.split() else "this topic"
        return (
            f"[SIMULATION] {topic.upper()} is a concept where the system uses "
            f"pre-defined rules to process input. The simulation maps known patterns "
            f"to responses. Unlike real AI, this does not learn — it just matches templates. "
            f"Score: {score_data['total_score']}/100."
        )
    elif any(w in prompt_lower for w in ["list", "give me", "show"]):
        return (
            f"[SIMULATION] Here is a template list:\n"
            f"1. Item A — predefined response\n"
            f"2. Item B — template-based\n"
            f"3. Item C — rule-matched\n"
            f"(Real AI would give actual, context-aware answers.)"
        )
    else:
        return (
            f"[SIMULATION] Prompt received. Score: {score_data['total_score']}/100. "
            f"The simulation engine matched your input to pattern #{hash(prompt) % 100}. "
            f"This is a pre-programmed response — no real understanding is happening here."
        )
